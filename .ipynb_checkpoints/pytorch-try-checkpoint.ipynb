{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ToxicDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        csv_path = os.path.join(root_dir, csv_file)\n",
    "        self.dataframe = pd.read_csv(csv_path)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataframe.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toxic_dataset = ToxicDataset(\"train.csv\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\n\\n Please do not vandalize pages, as you did with this edit to W. S. Merwin. If you continue to do so, you will be blocked from editing.    \"'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Deep Learning for NLP with Pytorch\n",
    "### Deep learning with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x104ec83d0>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for comments that aren't toxic\n",
    "def none_marker(x):\n",
    "    if x > 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "train['none'] = train.iloc[:,2:]\\\n",
    "    .sum(axis=1)\\\n",
    "    .apply(lambda x: none_marker(x))\n",
    "    \n",
    "# Remove multi-labels for now...\n",
    "multilabels_ix = train.iloc[:,2:]\\\n",
    "    .sum(axis=1)\\\n",
    "    .apply(lambda x: x < 2)\n",
    "train_one_label = train.loc[multilabels_ix,:]\n",
    "labels = train_one_label.iloc[:,2:].as_matrix()\n",
    "labels_ix = train_one_label.columns[2:]\n",
    "ix_1 = np.where(labels == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_str = np.array([labels_ix[i] for i in ix_1[1]])\n",
    "def transform_comment(comment):\n",
    "    result = comment\\\n",
    "        .translate(None, string.punctuation)\\\n",
    "        .lower()\\\n",
    "        .split()\n",
    "    return result\n",
    "comment_split = train_one_label['comment_text'].apply(lambda x: transform_comment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = zip(comment_split, labels_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split between train and test\n",
    "split = int(len(data) * 0.8)\n",
    "train = data[:split]\n",
    "test = data[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data) == (len(train) + len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert words to unique integer\n",
    "word_to_ix = {}\n",
    "for sent, _ in train + test:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "label_to_ix = {}\n",
    "for _, label in train + test:\n",
    "    if label not in label_to_ix:\n",
    "        label_to_ix[label] = len(label_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = len(label_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178059\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(VOCAB_SIZE)\n",
    "print(NUM_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        # Log Reg with input size of \"vocab_size\" and output of \"num_labels\"\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "    \n",
    "    def forward(self, bow_vec):\n",
    "        return F.log_softmax(self.linear(bow_vec), dim=1)\n",
    "\n",
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec.view(1, -1) # single row with many columns\n",
    "\n",
    "def make_target(label, label_to_ix):\n",
    "    return torch.LongTensor([label_to_ix[label]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      " 1.2211e-03 -1.0460e-03 -4.5942e-04  ...   1.9309e-03 -5.4459e-04  6.8789e-04\n",
      " 1.1627e-03 -2.2230e-03 -1.7261e-03  ...  -1.4355e-03  1.0937e-03  1.4988e-03\n",
      "-9.8975e-04 -2.5441e-04 -4.3393e-04  ...   1.9290e-03  1.2395e-03 -2.2232e-03\n",
      "-1.7802e-04  1.4664e-03 -1.1496e-03  ...   1.4417e-04 -1.0338e-03 -6.8515e-04\n",
      " 3.2670e-04 -8.0285e-04 -1.8626e-03  ...   6.6961e-04 -3.2703e-04 -7.3321e-06\n",
      "-3.8971e-04 -1.8819e-03  7.4758e-04  ...  -1.7646e-03 -1.1262e-03 -9.0539e-04\n",
      "[torch.FloatTensor of size 6x178059]\n",
      "\n",
      "Parameter containing:\n",
      "1.00000e-03 *\n",
      "  1.7748\n",
      " -0.6637\n",
      " -1.6992\n",
      "  0.1864\n",
      " -1.0139\n",
      " -1.2176\n",
      "[torch.FloatTensor of size 6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    # A = NUM_LABELS x VOCAB_SIZE = targets x inputs\n",
    "    # B = error\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-1.7879 -1.7888 -1.7971 -1.7907 -1.7954 -1.7907\n",
      "[torch.FloatTensor of size 1x6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = train[0]\n",
    "bow_vector = make_bow_vector(sample[0], word_to_ix)\n",
    "log_probs = model(autograd.Variable(bow_vector))\n",
    "print(log_probs) # obvious no merit results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-1.7876 -1.7915 -1.7875 -1.7955 -1.7976 -1.7910\n",
      "[torch.FloatTensor of size 1x6]\n",
      "\n",
      "Variable containing:\n",
      "-1.7813 -1.8028 -1.7982 -1.7842 -1.7923 -1.7919\n",
      "[torch.FloatTensor of size 1x6]\n",
      "\n",
      "Variable containing:\n",
      "-1.8065 -1.8157 -1.8029 -1.7431 -1.7858 -1.7982\n",
      "[torch.FloatTensor of size 1x6]\n",
      "\n",
      "Variable containing:\n",
      "-1.7961 -1.7895 -1.7924 -1.7953 -1.7908 -1.7865\n",
      "[torch.FloatTensor of size 1x6]\n",
      "\n",
      "Variable containing:\n",
      "-1.7863 -1.7934 -1.7953 -1.7930 -1.7963 -1.7864\n",
      "[torch.FloatTensor of size 1x6]\n",
      "\n",
      "Variable containing:\n",
      "-1.8049 -1.7848 -1.8054 -1.7878 -1.7835 -1.7845\n",
      "[torch.FloatTensor of size 1x6]\n",
      "\n",
      "Variable containing:\n",
      "-1.7972 -1.7883 -1.7780 -1.7963 -1.7956 -1.7952\n",
      "[torch.FloatTensor of size 1x6]\n",
      "\n",
      "Variable containing:\n",
      "-1.7951 -1.7970 -1.7866 -1.7858 -1.8200 -1.7667\n",
      "[torch.FloatTensor of size 1x6]\n",
      "\n",
      "Variable containing:\n",
      "-1.7844 -1.7810 -1.7867 -1.8195 -1.7989 -1.7807\n",
      "[torch.FloatTensor of size 1x6]\n",
      "\n",
      "Variable containing:\n",
      "-1.7808 -1.7922 -1.8100 -1.7866 -1.7966 -1.7847\n",
      "[torch.FloatTensor of size 1x6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction before training\n",
    "for sentence, label in test[:10]:\n",
    "    bow_vec = autograd.Variable(make_bow_vector(sentence, word_to_ix))\n",
    "    log_probs = model(bow_vec)\n",
    "    print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "1.00000e-03 *\n",
      " -0.2748\n",
      " -1.2245\n",
      "  0.5559\n",
      " -0.6667\n",
      "  0.3622\n",
      " -1.7945\n",
      "[torch.FloatTensor of size 6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print parameters for 'warmongering'\n",
    "print(next(model.parameters())[:, word_to_ix['warmongering']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define loss function and optimizing method\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "for epoch in range(100):\n",
    "    for sentence, label in train[:100]:\n",
    "    # for sentence, label in train:\n",
    "        # Step 1: clear accumulated gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Step 2: Make input and label\n",
    "        bow_vec = autograd.Variable(make_bow_vector(sentence, word_to_ix))\n",
    "        target = autograd.Variable(make_target(label, label_to_ix))\n",
    "        \n",
    "        # Step 3: Forward\n",
    "        log_probs = model(bow_vec)\n",
    "        \n",
    "        # Step 4: Backward\n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "for sentence, label in test[:10]:\n",
    "    bow_vec = autograd.Variable(make_bow_vector(sentence, word_to_ix))\n",
    "    log_probs = model(bow_vec)\n",
    "    # print(log_probs)\n",
    "    print(log_probs.data.numpy().argmax() == label_to_ix[label])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
